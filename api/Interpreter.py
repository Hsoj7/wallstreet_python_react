"""
Python interpreter for data generated by RedditScraper class created by Joshua Stone

https://www.reddit.com/r/wallstreetbets/comments/phcns0/weekend_discussion_thread_for_the_weekend_of/

to do:
    -Make the front end look nicer
    -Display cards of the top stocks, have all the information wanted for the stock
    -add sentiment meter: buy/sell + bull/bear. Look into tensor flow for this?
    -type in a word and have a sentiment based on that one company
    -Add crypto section
    -Save results into file in the evening each day. See what the likelihood of
    buying the top stock would generate revenue
"""

from RedditScraper import RedditScraper
from stockInfo import stockInfo
import os, os.path
from collections import Counter
import re

# WORK ON THIS IF YOU FIGURE OUT HOW TO SCRAPE THE DAILY DISCUSSION LINK
# def getURL():
#     discussionURL = "https://www.reddit.com/r/wallstreetbets/search?q=flair_name%3A%22Daily%20Discussion%22&restrict_sr=1&sort=new"
#     page = requests.get(discussionURL)
#
#     soup = BeautifulSoup(page.content, "html.parser")
#     # print(page.text)
#
#     return "yes"


# Function to instantiate the RedditScraper class and retrieve comments
def runRedditScraper():
    # print("Enter link to the subreddit: ")
    # # Get the link to the subreddit
    # url = input()

    #Update this line with newest subreddit daily discussion link
    url = "https://www.reddit.com/r/wallstreetbets/comments/pnzrg1/daily_discussion_thread_for_september_14_2021/"

    numFiles = len(os.listdir('./data_files'))
    numFiles = numFiles + 1
    filenameStr = "./data_files/data" + str(numFiles) + ".txt"

    # Create instance of RedditScraper class
    scrapeInstance = RedditScraper(url, filenameStr)
    scrapeInstance.scrape()

    print("SUCCESS: Data stored in data" + str(numFiles) + ".txt")

# Function to display the most common stock symbols
def getCommonNames():
    # print("Called getCommonNames GETTING CALLED TWICE, FIX")
    numFiles = len(os.listdir('./data_files'))
    numFiles = numFiles
    filenameStr = "./data_files/data" + str(numFiles) + ".txt"
    # Open data file generated from RedditScraper
    with open(filenameStr, 'r') as file:
        # Read file into data object
        data = file.read().replace('\n', ' ')
    file.close()
    data = re.sub('[^a-zA-Z0-9 \n]', ' ', data)
    # Split into individual words and count each word using imported Coutner library
    split = data.split()
    count = Counter(split)

    # Open common english words to filter out of result
    with open("./commonEnglish.txt", 'r') as file:
        commonEnglish = file.read().replace('\n', ' ')
    file.close()
    split2 = commonEnglish.split()

    symbolList = []

    # Get the 1000 most common words from the data generated
    most_common = count.most_common(1000)
    # Turn data into tuples of the word and their frequency
    strings, numbers = zip(*most_common)
    # Counts # of symbols printed so far
    NumSymbols = 0
    # Index for the tuple numbers object
    index = 0
    # Loop through the 1000 most common words
    for word in strings:
        # to print the 20 most common words, change if less/more are wanted
        if NumSymbols < 20:
            # Split2 contains the common english words
            if not word in split2:
                # Make sure ticker symbol is upper case and between 2-5 letters
                if word.isupper():
                    if len(word) > 1:
                        if len(word) < 6:
                            # Output the symbol if we've reached this far!
                            numPrinted = NumSymbols + 1
                                # scrapeInstance = RedditScraper(url, filenameStr)
                            # symbolClass = stockInfo(word, numbers[index])
                            # symbolList.append(symbolClass)
                            symbolList.append(word)
                            # print(str(numPrinted) + ": " + word + ", frequency: " + str(numbers[index]))
                            NumSymbols += 1
        else:
            break
        index += 1

    # for sym in symbolList:
    #     print("HERE " + sym.name)
    print("Returned symbolList ")
    return symbolList

    # return list of stocks...

# Main entry point
if __name__ == '__main__':
    #auto generates data[x].txt based on how many files are in the directory
    print("RAN MAIN")
    # numFiles = len(os.listdir('.')) - 4
    # filenameStr = "./data" + str(numFiles) + ".txt"
    # runRedditScraper()
    # getCommonNames()
    myString = getURL()
    print("getURL = " + myString)
